import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.DataInput;
import java.io.DataOutput;
import java.io.File;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.OutputStreamWriter;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.BitSet;
import java.util.Collections;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Set;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.filecache.DistributedCache;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.RawComparator;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.io.WritableComparator;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Partitioner;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.util.GenericOptionsParser;

public class BinarizationMR
{
  public static final String MISSING_INPUT_STR =  "MissingInputStr";
  public static final String MISSING_OUTPUT_STR =  "MissingOutputStr";
  public static final String OMITTED_COLUMNS =  "OmittedColumns";

  public static class MapClass extends Mapper<LongWritable, Text, Text, Text> 
  {
    private String key;
    private HashMap<Integer, List<String>> columnMetaData = new HashMap<Integer, List<String>>();
    int bitSetSize = 0;
    char missingOutputValue;
    char FIELD_SEPARATOR = ',';
    String COLUMN_META_FILE_SEPARATOR = ",";
    String INPUT_FILE_FIELD_SEPARATOR = "\t";

    // generate the Map of ColumnNames and their Distinct Values
    // So for example if the original file (Input file / table which needs to be
    // binarized -- is like this
    //      **** ASSUME - THE DATA FILE HAS NO HEADERS
    //  a1      b1      c1      d1    
    //  a2      b1      c1      d3    
    //  a1      b2      c2      d1    
    //  a3      b1      c4      d2    
    //  a1      b2      c3      d1    
    //  a2      b1      c1      d1    
    
    // input to the setup function below is
    // this input can be generated by another Map Reduce / JDBC program where it takes as input
    // the HIVE Table Name
    
    // 1,a1,a2,a3
    // 2,b1,b2
    // 3,c1,c2,c3,c4
    // 4,                       *** This indicates a Numeric Column ( which cannot be binarized )
    // 5,d1,d2,d3
    
    // Output of the setUp function below is like this a Data Structure with HashMap<String, LinkedList<String>>
    // 1 --> a1 a2  a3
    // 2 --> b1 b2  
    // 3 --> c1 c2  c3 c4
    // 4 -->
    // 5 --> d1 d2 d3
    
    @Override
    protected void setup(Context context)
        throws IOException, InterruptedException 
    {
      Path[] files = DistributedCache.getLocalCacheFiles(context.getConfiguration());
      missingOutputValue = context.getConfiguration().get(MISSING_OUTPUT_STR).charAt(0);
      BufferedReader br = null;
      try 
      {
        br = new BufferedReader(new FileReader(files[0].toString()));
        String line;
        int loc = 0;
        while ((line = br.readLine()) != null) 
        {
            StringTokenizer st =  new StringTokenizer(line, COLUMN_META_FILE_SEPARATOR);
            ArrayList<String> distinctColumnValues = new ArrayList<String>();
            int index = 0;
            while (st.hasMoreTokens())
                distinctColumnValues.add(index++, st.nextToken());
            Integer key = Integer.valueOf(distinctColumnValues.get(0));
            columnMetaData.put(key, distinctColumnValues.subList(1, distinctColumnValues.size()));
         }
         for(Integer k : columnMetaData.keySet())
         {
            int sz = columnMetaData.get(k).size();
            if (sz != 0)
                bitSetSize += sz;
         }
      } 
      finally 
      {
         if (br != null )
            br.close();
      }
    }
    
    // Data is coming in as - we assume the order of the columns in this the same as the
    // 
    //          a1      b1      c1      d1    
    //          a2      b1      c1      d3    
    //          a1      b2      c2      d1    
    //          a3      b1      c4      d2    
    //          a1      b2      c3      d1    
    //          a2      b1      c1      d1    
    
    @Override
    // key is the byteoffset - everything else is the value
    // we parse the value field into tokens separated by default tab
    public void map(LongWritable inKey, Text inValue, Context context) throws IOException, InterruptedException 
    {
      StringTokenizer itr = new StringTokenizer(inValue.toString(), INPUT_FILE_FIELD_SEPARATOR);
      int columnNumber = 1;
      StringBuilder numericColumns = new StringBuilder();
      char[] b = new char[bitSetSize];
      int locationInBitSet = 0;
      
      for(int s=0;s<bitSetSize;s++)
        b[s] = '0';
      
      System.out.println(inValue.toString());
      while (itr.hasMoreTokens()) 
      {
        key = itr.nextToken();
        System.out.println("Key " + key + ":");
        
        // get the location of the value occurring in the row for this column - with the index of the LinkedList in columnMetaData
        List<String> listOfValues = columnMetaData.get(new Integer(columnNumber));
        System.out.println("columnNumber " + columnNumber);
        System.out.println("listOfValues " + listOfValues);
        if(listOfValues != null && listOfValues.size() != 0)
        {
            // get the index where key occurs
            int ind = listOfValues.indexOf(key);
            int len = listOfValues.size();

            // IF Key is a Missing Character - then we replace all the occurrences of that column with OutputMissing Character
            if (ind == -1) // not in the list
            {
                for(int x=0;x<len;x++)
                    b[locationInBitSet+x] = missingOutputValue;
            }
            else
                b[locationInBitSet+ind] = '1';
            locationInBitSet += len;
        }
        else
            numericColumns.append(key).append(FIELD_SEPARATOR);
            
        columnNumber++;
       }
      
      // now the CharSet is ready - we create a Text representation of it
      StringBuilder sb = new StringBuilder();
      for(int s=0;s<bitSetSize;s++)
        sb.append(b[s]).append(FIELD_SEPARATOR);
      
      sb.append(numericColumns.toString()); // All Numeric Columns at the end
      Text t = new Text(sb.toString().substring(0, sb.toString().length()-1));
      context.write(t, new Text());
    }
  }

  // Mapper take 1 row of data - each separated by a whitespace
  // it generates ColumnNumber, String ( Content of that Column in the data for the row )
  // Example Data in
//  a1	b1	c1	d1    
//  a2	b1	c1	d3    
//  a1	b2	c2	d1    
//  a3	b1	c4	d2    
//  ?	b2	c3	d1    
//  a2	b1	c1	d1
  
  // Output of Mapper
  // 1      [a1, a2, a1, a3..]      ? is the missing character in input file
    public static class DistinctMapper extends Mapper<LongWritable, Text, IntWritable, Text > 
    {
        ArrayList<Integer> omitColumnList = new ArrayList<Integer>();
        
        @Override
        protected void setup(Context context) throws IOException, InterruptedException 
        {
            String omitCols = context.getConfiguration().get("OMITTED_COLUMNS");
            if ( omitCols != null)
            {
                StringTokenizer itr = new StringTokenizer(omitCols.toString());
                while (itr.hasMoreTokens()) 
                {
                    String colVal = itr.nextToken();
                    Integer outKey = Integer.valueOf(colVal);
                    omitColumnList.add(outKey);
                }
            }
        }
        
        @Override
        public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException 
        {
            StringTokenizer itr = new StringTokenizer(value.toString(), "\t");
            int columnNumber = 1;
            while (itr.hasMoreTokens()) 
            {
                String colVal = itr.nextToken();
                if ( !omitColumnList.contains(columnNumber) )
                {
                    IntWritable outKey = new IntWritable(columnNumber);
                    context.write(outKey, new Text(colVal.trim()));
                }
                columnNumber++;
            }
        }
    }
    
    // create a Set of Strings with the ColumnNumber as passed by the Mapper as Key
    // Incoming data into Reducer -- ColumnNumber, Stringvalues ( value in each column )
    // outgoing data from reducer -- ColumnerNumber, Contents of the Set of Strings
    // Output from Reducer
    //  1,a1, a2, a3
    //  2,b1, b2
    //  3,c1, c2, c4, c3
    //  4,d1, d2
    
    public static class DistinctReducer extends Reducer<IntWritable, Text, Text, NullWritable> 
    {
        String missingInputValue;
        String missingOutputValue;
        @Override
        protected void setup(Reducer.Context context) throws IOException, InterruptedException 
        {
            missingInputValue = context.getConfiguration().get(MISSING_INPUT_STR);
            missingOutputValue = context.getConfiguration().get(MISSING_OUTPUT_STR);
        }
        @Override
        protected void reduce(IntWritable key, Iterable<Text> values, Context context) throws IOException, InterruptedException 
        {
            Set<String> s = new HashSet<String>();
            for(Text t : values)
            {
                if ( t.find(missingInputValue) == -1)
                {                
                    String str = t.toString();
                    s.add(str.trim());
                }
            }
            StringBuilder sb = new StringBuilder();
            
            if (s.size() != 0)
                sb.append(key);
            for(String str1 : s)
                sb.append(",").append(str1.trim());
            context.write(new Text(sb.toString()), NullWritable.get());
        }
    }
  
  
  public static void main(String[] args) throws Exception 
  {
    char COLUMN_HEADER_FIELD_SEPARATOR = ',';
    Job job = new Job(new Configuration(), "Binarization.BinarizationMR");
    Configuration conf = job.getConfiguration();
    Job job1 = new Job(new Configuration(), "Binarization.BinarizationMR");
    Configuration conf1 = job1.getConfiguration();
    String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
    
    if (otherArgs.length < 2 ) 
    {
      System.err.println("Usage: BinarizationMR <in> <out> col2 col4...."); // integer numbers which indicate which columns to omit
      System.err.println("Input Path otherargs[0] = " + otherArgs[0]);
      System.err.println("Output Path otherargs[1] = " + otherArgs[1]);
      System.exit(2);
    }

    conf.set(MISSING_INPUT_STR, otherArgs[3]);
    conf.set(MISSING_OUTPUT_STR, otherArgs[4]);

    conf1.set(MISSING_INPUT_STR, otherArgs[3]);
    conf1.set(MISSING_OUTPUT_STR, otherArgs[4]);

    StringBuilder sb = new StringBuilder();
    for(int i=6;i<otherArgs.length;i++)
        sb.append(otherArgs[i]).append(" ");
    conf.set("OMITTED_COLUMNS", sb.toString());
    
    conf.set("mapred.textoutputformat.separator", " ");
    conf1.set("mapred.textoutputformat.separator", " ");
    job.setJarByClass(BinarizationMR.class);
    job1.setJarByClass(BinarizationMR.class);

    // Job for MetaData Generation
    job.setMapperClass(DistinctMapper.class);
    job.setReducerClass(DistinctReducer.class);

    // the map output is IntPair, IntWritable
    job.setMapOutputKeyClass(IntWritable.class);
    job.setMapOutputValueClass(Text.class);
    
    // so that all the metadata is in 1 file -- this will slow down 
    // for cases when there are a million columns in a table
    job.setNumReduceTasks(1);
   
    // // Job for Actual Binarization
    job1.setMapperClass(MapClass.class);

    // the map output is IntPair, IntWritable
    job1.setMapOutputKeyClass(Text.class);
    job1.setMapOutputValueClass(Text.class);
    job1.setNumReduceTasks(0);
    
    FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
    FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));
    
    FileInputFormat.addInputPath(job1, new Path(otherArgs[0]));
    FileOutputFormat.setOutputPath(job1, new Path(otherArgs[2]));
    
    if ( job.waitForCompletion(true))
    {
        System.out.println("===== Completed metadata generation =======");
        FileSystem fs = FileSystem.get(new Configuration());
        
        // Inout file - this is the ColumnMetaData Generate File ( from the 1st MR Code )
        String pathOfColumnMetaData = otherArgs[1] + "//part-r-00000";
        Path inFile = new Path(pathOfColumnMetaData);
        BufferedReader brMeta = new BufferedReader(new InputStreamReader(fs.open(inFile)));
        
        // Output File  - which has the column Headers
        String pathOfColumnHeader = otherArgs[1] + "//columnHeader.txt";
        Path outFile = new Path(pathOfColumnHeader);
        FSDataOutputStream out = fs.create(outFile);
        
        // name of the ColumnName input file
        String columnNameInFile = otherArgs[5];
        Path inFileColumnName = new Path(columnNameInFile);
        BufferedReader brColumName = new BufferedReader(new InputStreamReader(fs.open(inFileColumnName)));

        // populate the ColumnName List
        List<String> columnNameList = new ArrayList<String>();
        String lineColName=brColumName.readLine();
        int colNum = 0;
        while (lineColName != null)        
        {
            columnNameList.add(colNum++, lineColName.trim());
            lineColName = brColumName.readLine();
        }
        
        // Populate the ColumnMetaData Map
        String lineMeta=brMeta.readLine();
        StringBuilder columnNameString = new StringBuilder("");
        List<Integer> processedList = new ArrayList<Integer>();
        
        while (lineMeta != null)        
        {
            String[] meta = lineMeta.split(",");
            int columnNumber = Integer.parseInt(meta[0]);
            
            String columnName = columnNameList.get(columnNumber-1);
            processedList.add(columnNumber);
            for(int i=1;i<meta.length;i++)
                columnNameString.append(columnName + "_" + meta[i]).append(COLUMN_HEADER_FIELD_SEPARATOR);
            lineMeta=brMeta.readLine();
        }
        for(int i=0;i<columnNameList.size();i++)
            if (!processedList.contains(i+1))
                columnNameString.append(columnNameList.get(i)).append(COLUMN_HEADER_FIELD_SEPARATOR);
        String str = columnNameString.toString().trim().substring(0,columnNameString.length()-1);
        str = str + "\n";
        out.writeBytes(str);
        
        try
        {
            brMeta.close();
            brColumName.close();
            out.close();
        }
        catch(Exception e)
        {
            System.out.println("File Closing problems - "  + e);
        }

        
        DistributedCache.addCacheFile((new Path(pathOfColumnMetaData)).toUri(), conf1);
        if (job1.waitForCompletion(true))
        {
            System.out.println("===== Completed binarization =======");
        }
    }
  }
}

//	hadoop jar binarization.jar hivetableinput hivecolumnmetatest01 hivebinarizeouttest01 \N ? ColumnNames.txt 1 2 4 5 6 7 11 16 17 18 19 20 27
